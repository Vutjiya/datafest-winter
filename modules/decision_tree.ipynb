{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3897228c",
   "metadata": {},
   "source": [
    "# Decision Tree Module\n",
    "\n",
    "You may have noticed that while $k$-NN can work really well as a classifier, it doesn't provide us with any insights about the data itself. For example, $k$-NN doesn't tell us which features are most relevant in determining the class label of an observation. In this module, we introduce the decision tree model: a greedy, divide-and-conquer algorithm that partitions our feature space to create interpretable predictions.\n",
    "\n",
    "The decision tree model allows us to extract a set of _classification rules_ to classify a given instance. This _ruled-based_ ML approach is similar to creating `IF-ELSE` statements that test different features, resulting in a model that is highly interpretable by humans. Unlike $k$-NN, which acts as a \"black box\" that simply outputs predictions, decision trees show us exactly how they arrive at their conclusions through a series of logical decisions.\n",
    "\n",
    "Let's make these claims more concrete with a familiar example. A common parlor game is $20$ Questions, where one person chooses something for the other players to guess. The guessers are allowed to ask $20$ yes-or-no questions to identify what was chosen. If you've ever played this game, you know that the best strategy is to ask questions that give you the most information possible. For instance, if you're trying to guess an animal, asking \"Is it a mammal?\" is far more informative than asking \"Is it a cat?\" The first question immediately divides all animals into two groups, eliminating roughly half of the possibilities with a single question. The second question only helps if the answer happens to be \"yes.\" Decision trees work in exactly this way: at each step, they choose the feature and threshold that best splits the data into distinct groups, progressively narrowing down the possibilities until an accurate prediction can be made.\n",
    "\n",
    "This process of sequential splitting creates a tree-like structure where each internal node represents a test on a feature, each branch represents the outcome of that test, and each leaf node represents a class label. By following the path from the root to a leaf, we can see exactly which conditions led to a particular classification. This makes decision trees one of the most transparent ML models.\n",
    "\n",
    "talk about robustness and variance of trees\n",
    "\n",
    "talk about pruning and cut off criteria\n",
    "\n",
    "structure of model depends on data, unlike other models\n",
    "\n",
    "cost complexity pruning\n",
    "\n",
    "look at decision boundary\n",
    "\n",
    "use mermaid diagram for decision trees\n",
    "\n",
    "use house votes for tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import seaborn.objects as so\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.datasets import make_circles\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827c1ac",
   "metadata": {},
   "source": [
    "In scikit-learn, a decision tree classifier can be fit using the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "depths = [3, 5, 10, 20]\n",
    "\n",
    "h = 0.02\n",
    "x_min, x_max = -0.1, 4.1\n",
    "y_min, y_max = -0.1, 4.1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "X, y = make_circles(n_samples=100000, noise=0.1, factor=0.5, random_state=42)\n",
    "X = (X + 1) * 2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "for ax, depth in zip(axes, depths):\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    Z = dt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.scatter(xx, yy, c=Z, cmap=\"RdYlGn\", alpha=0.8, s=1, marker=\".\")\n",
    "\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_xlabel(r\"$x_1$\")\n",
    "    ax.set_ylabel(r\"$x_2$\")\n",
    "    ax.set_title(f\"Decision Tree: depth = {depth}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138742fe",
   "metadata": {},
   "source": [
    "Based on our formulation of the decision tree algorithm, you may have noticed that the only stopping criteria for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d973af08",
   "metadata": {},
   "source": [
    "# Computational Considerations\n",
    "\n",
    "searching over entire space of decision trees given dataset is NP-hard\n",
    "\n",
    "hypothesis space of decision trees is space of all boolean functions\n",
    "\n",
    "decision tree algorithm is form of hill-climbing algorithm\n",
    "\n",
    "have to use greedy algorithm since computationally infeasible to find best accuracy wrt entire dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e059e3",
   "metadata": {},
   "source": [
    "# Titanic\n",
    "\n",
    "Here, we'll be using the Titanic dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pl.read_csv(\"./data/titanic.csv\")\n",
    "\n",
    "X, y = titanic[:, 1:], titanic[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = pl.read_parquet(\"./data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c27c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Access the bytes field from the struct\n",
    "image_bytes = mnist[1, \"image\"][\"bytes\"]\n",
    "\n",
    "# Decode the image bytes using PIL\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = np.sin(X) + 0.1 * np.random.randn(100, 1)\n",
    "\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 20), nn.Tanh(), nn.Linear(20, 20), nn.Tanh(), nn.Linear(20, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.scatter(X, y, alpha=0.5, s=10, label=\"True data\")\n",
    "(line,) = ax1.plot([], [], \"r-\", linewidth=2, label=\"NN prediction\")\n",
    "ax1.set_xlim(-3, 3)\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.set_title(\"Neural Network Learning\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "loss_history = []\n",
    "ax2.set_xlim(0, 200)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Training Loss\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "(loss_line,) = ax2.plot([], [], \"b-\", linewidth=2)\n",
    "\n",
    "epoch_text = fig.text(0.5, 0.95, \"\", ha=\"center\", fontsize=12, weight=\"bold\")\n",
    "\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    loss_line.set_data([], [])\n",
    "    return line, loss_line, epoch_text\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_tensor)\n",
    "    loss = criterion(predictions, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_tensor).numpy()\n",
    "    line.set_data(X, y_pred)\n",
    "\n",
    "    loss_line.set_data(range(len(loss_history)), loss_history)\n",
    "    if len(loss_history) > 50:\n",
    "        ax2.set_ylim(0, max(loss_history[:50]))\n",
    "\n",
    "    epoch_text.set_text(f\"Epoch: {frame} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return line, loss_line, epoch_text\n",
    "\n",
    "\n",
    "anim = FuncAnimation(\n",
    "    fig, update, frames=200, init_func=init, blit=True, interval=50, repeat=False\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "anim.save(\"neural_network_learning.gif\", writer=\"pillow\", fps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d3a16",
   "metadata": {},
   "source": [
    "![Alt text for the GIF](../assets/neural_network_learning.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b8b0f",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A([Start: Is the issue resolved?])\n",
    "    B{Is the internet working?}\n",
    "    C[Try rebooting the modem.]\n",
    "    D((Issue resolved))\n",
    "    E((Call ISP))\n",
    "\n",
    "    A -->|Yes| D\n",
    "    A -->|No| B\n",
    "    B -->|Yes| C\n",
    "    B -->|No| E\n",
    "    C --> A\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafest-winter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
