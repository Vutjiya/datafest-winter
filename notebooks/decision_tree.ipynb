{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256fd46b",
   "metadata": {},
   "source": [
    "talk about robustness and variance of trees\n",
    "\n",
    "talk about pruning and cut off criteria\n",
    "\n",
    "structure of model depends on data, unlike other models\n",
    "\n",
    "cost complexity pruning\n",
    "\n",
    "look at decision boundary\n",
    "\n",
    "use mermaid diagram for decision trees\n",
    "\n",
    "use house votes for tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897228c",
   "metadata": {},
   "source": [
    "# Decision Tree Module\n",
    "\n",
    "You may have noticed that while $k$-NN can work really well as a classifier, it doesn't provide us with any insights about the data itself. For example, $k$-NN doesn't tell us which features are most relevant in determining the class label of an observation. In this module, we introduce the decision tree model: a greedy, divide-and-conquer algorithm that partitions our feature space to create interpretable predictions.\n",
    "\n",
    "The decision tree model allows us to extract a set of _classification rules_ to classify a given instance. This _ruled-based_ ML approach is similar to creating `IF-ELSE` statements that test different features, resulting in a model that is highly interpretable by humans. Unlike $k$-NN, which acts as a \"black box\" that simply outputs predictions, decision trees show us exactly how they arrive at their conclusions through a series of logical decisions.\n",
    "\n",
    "Let's make these claims more concrete with a familiar example. A common parlor game is $20$ Questions, where one person chooses something for the other players to guess. The guessers are allowed to ask $20$ yes-or-no questions to identify what was chosen. If you've ever played this game, you know that the best strategy is to ask questions that give you the most information possible. For instance, if you're trying to guess an animal, asking \"Is it a mammal?\" is far more informative than asking \"Is it a cat?\" The first question immediately divides all animals into two groups, eliminating roughly half of the possibilities with a single question. The second question only helps if the answer happens to be \"yes.\" \n",
    "\n",
    "So clearly, we should ask if it's a mammal before asking if it's a cat. Now that we have a notion that there are certain questions that are more useful than others, and that there's an ordering to asking these questions, this begs the question: which questions exactly to ask first and how do we quantify how useful each question is in order to chose the best one to ask. For this, we borrow a few ideas from information theory: entropy and information gain.\n",
    "\n",
    "We define information gain as\n",
    "\n",
    "\n",
    "\n",
    "Decision trees work in exactly this way: at each step, they choose the feature and threshold that best splits the data into distinct groups, progressively narrowing down the possibilities until an accurate prediction can be made.\n",
    "\n",
    "This process of sequential splitting creates a tree-like structure where each internal node represents a test on a feature, each branch represents the outcome of that test, and each leaf node represents a class label. By following the path from the root to a leaf, we can see exactly which conditions led to a particular classification. This makes decision trees one of the most transparent ML models.\n",
    "\n",
    "finish introducing trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dab9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Note: run this cell first to import necessary packages\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = (\n",
    "    str(Path.cwd().parent) if \"notebooks\" in str(Path.cwd()) else str(Path.cwd())\n",
    ")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils import plot_boundary, plot_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827c1ac",
   "metadata": {},
   "source": [
    "In scikit-learn, a decision tree classifier can be fit using the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). The code for fitting a decision tree is pretty much the same as for the $k$-NN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12858cc",
   "metadata": {},
   "source": [
    "# House Votes 84\n",
    "For this excerise, we'll be working with the []()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "depths = [3, 5, 10, 20]\n",
    "\n",
    "h = 0.02\n",
    "x_min, x_max = -0.1, 4.1\n",
    "y_min, y_max = -0.1, 4.1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "X, y = make_circles(n_samples=100000, noise=0.1, factor=0.5, random_state=42)\n",
    "X = (X + 1) * 2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "for ax, depth in zip(axes, depths):\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    Z = dt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.scatter(xx, yy, c=Z, cmap=\"RdYlGn\", alpha=0.8, s=1, marker=\".\")\n",
    "\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_xlabel(r\"$x_1$\")\n",
    "    ax.set_ylabel(r\"$x_2$\")\n",
    "    ax.set_title(f\"Decision Tree: depth = {depth}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138742fe",
   "metadata": {},
   "source": [
    "Based on our formulation of the decision tree algorithm, you may have noticed that the only stopping criteria for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d973af08",
   "metadata": {},
   "source": [
    "# Computational Considerations\n",
    "\n",
    "searching over entire space of decision trees given dataset is NP-hard\n",
    "\n",
    "hypothesis space of decision trees is space of all boolean functions\n",
    "\n",
    "decision tree algorithm is form of hill-climbing algorithm\n",
    "\n",
    "have to use greedy algorithm since computationally infeasible to find best accuracy wrt entire dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e059e3",
   "metadata": {},
   "source": [
    "# Titanic\n",
    "\n",
    "Here, we'll be using the Titanic dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pl.read_csv(\"./data/titanic.csv\")\n",
    "\n",
    "X, y = titanic[:, 1:], titanic[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = pl.read_parquet(\"./data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c27c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Access the bytes field from the struct\n",
    "image_bytes = mnist[1, \"image\"][\"bytes\"]\n",
    "\n",
    "# Decode the image bytes using PIL\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = np.sin(X) + 0.1 * np.random.randn(100, 1)\n",
    "\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 20), nn.Tanh(), nn.Linear(20, 20), nn.Tanh(), nn.Linear(20, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.scatter(X, y, alpha=0.5, s=10, label=\"True data\")\n",
    "(line,) = ax1.plot([], [], \"r-\", linewidth=2, label=\"NN prediction\")\n",
    "ax1.set_xlim(-3, 3)\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.set_title(\"Neural Network Learning\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "loss_history = []\n",
    "ax2.set_xlim(0, 200)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Training Loss\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "(loss_line,) = ax2.plot([], [], \"b-\", linewidth=2)\n",
    "\n",
    "epoch_text = fig.text(0.5, 0.95, \"\", ha=\"center\", fontsize=12, weight=\"bold\")\n",
    "\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    loss_line.set_data([], [])\n",
    "    return line, loss_line, epoch_text\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_tensor)\n",
    "    loss = criterion(predictions, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_tensor).numpy()\n",
    "    line.set_data(X, y_pred)\n",
    "\n",
    "    loss_line.set_data(range(len(loss_history)), loss_history)\n",
    "    if len(loss_history) > 50:\n",
    "        ax2.set_ylim(0, max(loss_history[:50]))\n",
    "\n",
    "    epoch_text.set_text(f\"Epoch: {frame} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return line, loss_line, epoch_text\n",
    "\n",
    "\n",
    "anim = FuncAnimation(\n",
    "    fig, update, frames=200, init_func=init, blit=True, interval=50, repeat=False\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "anim.save(\"neural_network_learning.gif\", writer=\"pillow\", fps=20)j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d3a16",
   "metadata": {},
   "source": [
    "![Alt text for the GIF](../assets/neural_network_learning.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c080a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 5px; display: block; margin-left:auto; margin-right:auto; width: fit-content; zoom: 2;\">\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#e3f2fd','primaryTextColor':'#000','primaryBorderColor':'#1976d2','lineColor':'#000','secondaryColor':'#fff3e0','tertiaryColor':'#c8e6c9','edgeLabelBackground':'#ffffff', 'fontSize':'20px'}, 'flowchart': {'nodeSpacing': 100, 'rankSpacing': 100}}}%%\n",
    "flowchart TD\n",
    "    A[Age] --> |Young| B[Student]\n",
    "    A --> |Middle Age| C[Yes]\n",
    "    A --> |Older Adult| D[Credit_Score]\n",
    "    \n",
    "    B -->|No| E[No]\n",
    "    B -->|Yes| F[Yes]\n",
    "    \n",
    "    D -->|Regular| G[No]\n",
    "    D -->|Excellent| H[Yes]\n",
    "    \n",
    "    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n",
    "    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n",
    "    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n",
    "    style C fill:#c8e6c9,stroke:#388e3c,stroke-width:2px,color:#000\n",
    "    style E fill:#c8e6c9,stroke:#388e3c,stroke-width:2px,color:#000\n",
    "    style F fill:#c8e6c9,stroke:#388e3c,stroke-width:2px,color:#000\n",
    "    style G fill:#c8e6c9,stroke:#388e3c,stroke-width:2px,color:#000\n",
    "    style H fill:#c8e6c9,stroke:#388e3c,stroke-width:2px,color:#000\n",
    "    \n",
    "    linkStyle 0,1,2,3,4,5,6 stroke:#000,color:#000\n",
    "```\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafest-winter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
